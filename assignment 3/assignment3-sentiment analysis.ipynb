{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab63e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install findspark\n",
    "pip install spark-nlp==4.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a485cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2255f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/h_qf_0c97tq0_jbwkb12qrlm0000gn/T/ipykernel_2747/4144994439.py:7: UserWarning: SparkContext already exists in this scope\n",
      "  warnings.warn(\"SparkContext already exists in this scope\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = ps.SparkContext('local[2]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4104c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb4bd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.44.49.91:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84f9c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.44.49.91:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e3c7820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb6fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 19:18:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:<version>\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a041e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 4.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version:\", sparknlp.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb80f67",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 19:18:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----------------------------------+-----+\n",
      "|review_id| app_id|                        review_text|label|\n",
      "+---------+-------+-----------------------------------+-----+\n",
      "|138086736|1742020|               Lemme get a Crunc...|    1|\n",
      "|138083242|1293460|               [h1] Less of a ca...|    1|\n",
      "|138125042|1321440|               Cassette Beasts i...|    1|\n",
      "|138076943|1575830|               I'm really not su...|    0|\n",
      "|138082413|1159690|               Voidtrain is a ne...|    1|\n",
      "|137685823|1321440|               Playtime: 20 hour...|    1|\n",
      "|137905799|1321440|仅代表个人观点，浅浅的打个分吧~\\...|    1|\n",
      "|123764026|2027560|               It's close to bar...|    1|\n",
      "|138091114|1742020|               Soratomo, Robosa,...|    1|\n",
      "|137542841|2176930|               https://steamcomm...|    1|\n",
      "|138079463|1940340|               The good:\\n- Rela...|    0|\n",
      "|138082319|1940340|               Alright, so this ...|    0|\n",
      "|138093108|2229260|               I played Eden Ete...|    1|\n",
      "|137684311|1321440|               I wasn't expectin...|    1|\n",
      "|138125985|1159690|               A friend of mine ...|    1|\n",
      "|138126546|1159690|               This game is incr...|    1|\n",
      "|138090218|1940340|               Interesting seque...|    1|\n",
      "|138090218|1940340|               Interesting seque...|    1|\n",
      "|138126222|2069040|               I love puzzle gam...|    1|\n",
      "|138123827|1230170|               I've largely enjo...|    1|\n",
      "+---------+-------+-----------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 建立 SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP Example\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "# 定義 JSON 檔案的模式\n",
    "schema = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 設定包含 JSON 檔案的根目錄路徑\n",
    "root_path = '/Users/bijiben/Desktop/BDA/advancedanalytics/assignment3/data/*/*'\n",
    "\n",
    "# 讀取並合併所有 JSON 檔案\n",
    "df = spark.read.schema(schema).json(root_path)\n",
    "\n",
    "# 顯示讀取結果\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eca508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696f263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+\n",
      "|review_id| app_id|         review_text|label|\n",
      "+---------+-------+--------------------+-----+\n",
      "|138086736|1742020|Lemme get a Crunc...|    1|\n",
      "|138083242|1293460|[h1] Less of a ca...|    1|\n",
      "|138125042|1321440|Cassette Beasts i...|    1|\n",
      "|138076943|1575830|I'm really not su...|    0|\n",
      "|138082413|1159690|Voidtrain is a ne...|    1|\n",
      "+---------+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be745ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().dropDuplicates()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac15fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.8, 0.1, 0.1], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04c407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try preprocessing with sparknlp\n",
    "import sparknlp\n",
    "spark= sparknlp.start()\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml.feature import (\n",
    "    DocumentAssembler,\n",
    "    Tokenizer,\n",
    "    Normalizer,\n",
    "    StopWordsCleaner,\n",
    "    TokenAssembler,\n",
    "    Stemmer,\n",
    "    Lemmatizer\n",
    ")\n",
    "\n",
    "documentAssembler= DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "tokenizer= Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer= Normalizer()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"])\n",
    "\n",
    "stopwordsCleaner =StopWordsCleaner()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"cleaned_tokens\")\\\n",
    "    .setCaseSensitive(True)\n",
    "\n",
    "tokenAssembler= TokenAssembler()\\\n",
    "    .setInputCols([\"sentence\", \"cleaned_tokens\"])\\\n",
    "    .setOutputCol(\"assembled\")\n",
    "\n",
    "stemmer= Stemmer()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "lemmatizer= Lemmatizer()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"lemma\")\\\n",
    "    .setDictionary(\"AntBNC_lemmas_ver_001.txt\",  \n",
    "    value_delimiter=\"\\t\", key_delimiter=\"->\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler,\n",
    "    tokenizer,\n",
    "    sentenceDetector,\n",
    "    normalizer,\n",
    "    stopwordsCleaner,\n",
    "    tokenAssembler,\n",
    "    stemmer,\n",
    "    lemmatizer, tokenizer, cv, idf, lr])\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[preprocess_transformer, replace_transformer, translate_transformer, tokenizer, cv, idf, lr])\n",
    "\n",
    "TF_pipelineFit = pipeline.fit(train_set)\n",
    "train_df = TF_pipelineFit.transform(train_set)\n",
    "val_df = TF_pipelineFit.transform(val_set)\n",
    "train_df.show(5)\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "import sparknlp\n",
    "spark= sparknlp.start()\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01,elasticNetParam=1.0)\n",
    "#pipeline = Pipeline(stages=[tokenizer, cv, idf, lr])\n",
    "pipeline = Pipeline(stages=[preprocess_transformer, replace_transformer, translate_transformer, tokenizer, cv, idf, lr])\n",
    "#CV_pipelineFit = pipeline.fit(df).transform(df)\n",
    "\n",
    "CV_pipelineFit = pipeline.fit(train_set)\n",
    "predictions = CV_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3649d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 139:==========================================>            (14 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 21:11:11 WARN DAGScheduler: Broadcasting large task binary with size 1092.9 KiB\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|review_id| app_id|         review_text|label|               words|                  tf|            features|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|123764026|2027560|It's close to bar...|    1|[it's, close, to,...|(65536,[104,632,8...|(65536,[104,632,8...|\n",
      "|123779352|2027560|If you're into bu...|    1|[if, you're, into...|(65536,[1696,1880...|(65536,[1696,1880...|\n",
      "|123780055|2027560|Took the game bec...|    1|[took, the, game,...|(65536,[110,495,1...|(65536,[110,495,1...|\n",
      "|123780785|2027560|Cute game with re...|    1|[cute, game, with...|(65536,[3584,7823...|(65536,[3584,7823...|\n",
      "|123821893|2027560|This game is less...|    1|[this, game, is, ...|(65536,[1540,1880...|(65536,[1540,1880...|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 22:00:08 WARN TransportChannelHandler: Exception in connection from /10.44.49.91:52330\n",
      "java.io.IOException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# HashingTF + IDF + Logistic Regression\n",
    "#Accuracy Score: 0.8666666666666667\n",
    "#ROC-AUC: 0.8529411764705882\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "# label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"target_label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf])\n",
    "\n",
    "TF_pipelineFit = pipeline.fit(train_set)\n",
    "train_df = TF_pipelineFit.transform(train_set)\n",
    "val_df = TF_pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d96b716",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 19:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1095.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 19:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/19 19:20:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/19 19:20:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/19 19:20:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/05/19 19:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:21 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:23 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:24 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n",
      "23/05/19 19:20:26 WARN DAGScheduler: Broadcasting large task binary with size 1097.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:=======================================>               (13 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 19:20:30 WARN DAGScheduler: Broadcasting large task binary with size 1116.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c754759",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CountVectorizer + IDF + Logistic Regression\n",
    "##crossvalidation to select hyperparamter\n",
    "#Accuracy Score: 0.8529\n",
    "#ROC-AUC: 0.8917\n",
    "# %%time\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "lr = Logfrom pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "isticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, lr])\n",
    "\n",
    "# 定義參數網格用於交叉驗證\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# 建立交叉驗證器\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# 使用訓練數據訓練模型\n",
    "CV_pipelineFit = crossval.fit(train_set)\n",
    "\n",
    "\n",
    "predictions = CV_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print (\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#之前成功没有前处理的代码\n",
    "# CountVectorizer + IDF + Logistic Regression\n",
    "#Accuracy Score: 0.8529\n",
    "#ROC-AUC: 0.8917\n",
    "# %%time\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "import sparknlp\n",
    "spark= sparknlp.start()\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01,elasticNetParam=1.0)\n",
    "#pipeline = Pipeline(stages=[tokenizer, cv, idf, lr])\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, lr])\n",
    "#CV_pipelineFit = pipeline.fit(df).transform(df)\n",
    "\n",
    "CV_pipelineFit = pipeline.fit(train_set)\n",
    "predictions = CV_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "print (\"Accuracy Score: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb854cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters:\n",
    "#regParam: 0.01\n",
    "#elasticNetParam: 1.0\n",
    "bestModel = CV_pipelineFit.bestModel\n",
    "bestLRModel = bestModel.stages[-1]  # 取得最佳的邏輯回歸模型\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(\"regParam:\", bestLRModel.getRegParam())\n",
    "print(\"elasticNetParam:\", bestLRModel.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9797e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score: 0.7407\n",
    "#ROC-AUC: 0.7568\n",
    "test_df = TF_pipelineFit.transform(test_set)\n",
    "test_predictions = lrModel.transform(test_df)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "# print accuracy, roc_auc\n",
    "print (\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print (\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CountVectorizer + IDF + Logistic Regression\n",
    "\n",
    "#Accuracy Score: 0.7963\n",
    "#ROC-AUC: 0.7591\n",
    "test_predictions = CV_pipelineFit.transform(test_set)\n",
    "accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "roc_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print (\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print (\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edebfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "CV_pipelineFit.save(\"CV_pipelineFit.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ff833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
